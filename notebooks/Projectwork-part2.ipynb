{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5afa81-248e-44c2-9067-139d78c6839f",
   "metadata": {},
   "source": [
    "# Project work, part 2 - Data Sources\n",
    "### Orie Kimura\n",
    "##### October 24, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d8760-21ab-4393-962d-d514d9f676fc",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433df919-6d4f-466b-a5b6-e6a6754deeb6",
   "metadata": {},
   "source": [
    "Links to your public GitHub repository and Streamlitapp for the compulsory work.  \n",
    "\n",
    "https://github.com/oriekimura123/IND320-Projectwork  \n",
    "https://oriekimura-ind320-projectwork.streamlit.app/   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b494e7",
   "metadata": {},
   "source": [
    "## Tasks  \n",
    "### Accounts and repositories  \n",
    "Both acounts and repositories are created and resed.  \n",
    "\n",
    "### Local database: Cassandra\n",
    "Spark-Cassandra connection works and the Cassandra database can be accessed from the Jupyter Notebook and uset to store data from the API.  \n",
    "\n",
    "### Remote database: MongoDB  \n",
    "Tested that I can manupilate data from Python.  \n",
    "The MongoDB database stores data that has been trimmed/curated/prepared through the Jupyter Notebook and Spark filtering.  \n",
    "These data can be accessed directly from the Streamlit app.  \n",
    "\n",
    "### API  \n",
    "the API connection at https://api.elhub.no  \n",
    "I am goitng to get data from https://api.elhub.no/energy-data-api#/price-areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78ab7a-ae15-40f6-a4cb-1b9530a9a826",
   "metadata": {},
   "source": [
    "## Tasks, Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1f02d-3d48-4fa8-a831-f82d024ebdc1",
   "metadata": {},
   "source": [
    "1. Use the Elhub API to retrieve hourly production data for all price areas using PRODUCTION_PER_GROUP_MBA_HOUR for all days and hours of the year 2021.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63b0de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables set successfully\n"
     ]
    }
   ],
   "source": [
    "# --- Environment Setup ---\n",
    "import os\n",
    "\n",
    "# Paths configuration\n",
    "SPARK_HOME = \"C:\\\\Spark\\\\spark-3.5.1-bin-hadoop3\"\n",
    "HADOOP_HOME = \"C:\\\\Hadoop\\\\hadoop-3.3.1\"\n",
    "JAVA_HOME = \"C:\\\\Program Files\\\\Microsoft\\\\jdk-21.0.8.9-hotspot\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ.update({\n",
    "    \"SPARK_HOME\": SPARK_HOME,\n",
    "    \"HADOOP_HOME\": HADOOP_HOME,\n",
    "    \"JAVA_HOME\": JAVA_HOME,\n",
    "    \"PATH\": os.environ[\"PATH\"] + os.pathsep + os.path.join(SPARK_HOME, \"bin\"),\n",
    "    \"PYSPARK_PYTHON\": \"python\",\n",
    "    \"PYSPARK_DRIVER_PYTHON\": \"python\",\n",
    "    \"PYSPARK_HADOOP_VERSION\": \"without\",\n",
    "    \"SPARK_CONF_DIR\": os.path.join(SPARK_HOME, \"conf\")\n",
    "})\n",
    "\n",
    "# Database configuration\n",
    "CASSANDRA_KEYSPACE = \"my_keyspace\"\n",
    "CASSANDRA_TABLE = \"production_data\"\n",
    "MONGO_DATABASE = \"elhub_data\"\n",
    "MONGO_COLLECTION = \"production_data_hourly\"\n",
    "\n",
    "print(\"Environment variables set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c08d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "C:\\Users\\oriek\\miniconda3\\envs\\DVD_new\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.elhub.no'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fetch data from api.elhub.no for all 5 price areas in Norway for the year 2021\n",
    "import requests\n",
    "import json\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from urllib.parse import quote\n",
    "from datetime import date\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL without the specific price area\n",
    "BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "\n",
    "# Define the dataset parameter separately\n",
    "DATASET_PARAM = \"dataset=PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "\n",
    "# List of all five Norwegian price areas\n",
    "PRICE_AREAS: List[str] = [\"NO1\", \"NO2\", \"NO3\", \"NO4\", \"NO5\"] \n",
    "\n",
    "# Define the start and end of the full year\n",
    "START_YEAR = date(2021, 1, 1)\n",
    "END_YEAR = date(2021, 12, 31)\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "DELAY_SECONDS = 0.5\n",
    "\n",
    "# --- Main Data Storage ---\n",
    "all_data: List[Dict] = []\n",
    "total_records_collected = 0\n",
    "\n",
    "# --- Start Iterating Through Each Price Area ---\n",
    "for area in PRICE_AREAS:\n",
    "    # print(f\"\\n==================== Starting Fetch for AREA: {area} ====================\")\n",
    "    current_start_date = START_YEAR\n",
    "    \n",
    "    # Loop month by month for the current area\n",
    "    while current_start_date <= END_YEAR:\n",
    "        # Calculate the monthly date range\n",
    "        next_month_start = current_start_date + relativedelta(months=1)\n",
    "        current_end_date = next_month_start - relativedelta(days=1)\n",
    "        \n",
    "        if current_end_date > END_YEAR:\n",
    "            current_end_date = END_YEAR\n",
    "\n",
    "        # Define timezone (+01:00 for Norway, UTC+1)\n",
    "        TZ_OFFSET = timezone(timedelta(hours=1))\n",
    "\n",
    "        # Convert date (which has no time) to datetime with timezone\n",
    "        start_dt = datetime.combine(current_start_date, datetime.min.time(), tzinfo=TZ_OFFSET)\n",
    "        end_dt = datetime.combine(current_end_date + timedelta(days=1), datetime.min.time(), tzinfo=TZ_OFFSET)\n",
    "\n",
    "        # Create proper ISO-8601 timestamps and encode them\n",
    "        start_date_str = quote(start_dt.isoformat(), safe=\"\")\n",
    "        end_date_str = quote(end_dt.isoformat(), safe=\"\")\n",
    "\n",
    "        # Construct the full API URL\n",
    "        url = (\n",
    "            f\"{BASE_URL}/{area}?{DATASET_PARAM}\"\n",
    "            f\"&startDate={start_date_str}&endDate={end_date_str}\"\n",
    "        )\n",
    "\n",
    "        # print(f\"-> Requesting data for {start_date_str} to {end_date_str} in {area}...\")\n",
    "\n",
    "        # --- Retry Logic ---\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                if attempt > 0:\n",
    "                    print(f\"   (Attempt {attempt + 1}/{MAX_RETRIES}) Retrying in {DELAY_SECONDS}s...\")\n",
    "                    time.sleep(DELAY_SECONDS)\n",
    "                \n",
    "                # Make the API request\n",
    "                response = requests.get(url, verify=False)\n",
    "                response.raise_for_status() \n",
    "\n",
    "                # Extract and process data\n",
    "                data = response.json().get('data', [])\n",
    "                \n",
    "                # Add the priceArea field to each record before appending, to ensures the data has the priceArea column for Spark analysis later\n",
    "                for record in data:\n",
    "                    record['priceArea'] = area\n",
    "                    \n",
    "                all_data.extend(data)\n",
    "                total_records_collected += len(data)\n",
    "                \n",
    "                # print(f\"<- Successfully retrieved {len(data)} records for {area}.\")\n",
    "                break # Success, move to the next month\n",
    "\n",
    "            except requests.exceptions.HTTPError as errh:\n",
    "                print(f\"HTTP Error for {area} ({start_date_str}): {errh}\")\n",
    "                if response.status_code < 500 or attempt == MAX_RETRIES - 1:\n",
    "                    break # Stop retrying on client errors (4xx) or max attempts\n",
    "            except requests.exceptions.RequestException as err:\n",
    "                print(f\"General Request Error for {area} ({start_date_str}): {err}\")\n",
    "                if attempt == MAX_RETRIES - 1:\n",
    "                    break\n",
    "\n",
    "        # Move to the next month\n",
    "        current_start_date = next_month_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2630846c-5992-4c3f-b088-dff0de539c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is written to: C:\\Users\\oriek\\IND320\\IND320-Projectwork-Orie\\downloads\\Production_per_group_mad_hours_2021.json\n"
     ]
    }
   ],
   "source": [
    "# download the data as a JSON file to the downloads folder\n",
    "import json\n",
    "import os\n",
    "\n",
    "repo_root = os.path.abspath(os.path.join(os.path.dirname(''), '..')) \n",
    "\n",
    "DOWNLOADS_FOLDER = os.path.join(repo_root, 'downloads')\n",
    "FILE_PATH = os.path.join(DOWNLOADS_FOLDER, 'Production_per_group_mad_hours_2021.json')\n",
    "\n",
    "# Make sure the folder exists (Important!)\n",
    "os.makedirs(DOWNLOADS_FOLDER, exist_ok=True)\n",
    "\n",
    "# Write JSON to file using the ABSOLUTE path\n",
    "with open(FILE_PATH, 'w') as f:\n",
    "    json.dump(all_data, f, indent=4) \n",
    "\n",
    "print(f\"The file is written to: {FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2819f9a-cb87-4418-b7e2-b55a78724c7b",
   "metadata": {},
   "source": [
    "Extract only the list in productionPerGroupMbaHour, convert to a DataFrame, and insert the data into Cassandra using Spark.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058e6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all productionPerGroupMbaHour lists from all_data and flatten into a single list\n",
    "\n",
    "production_data_list = []\n",
    "for record in all_data:\n",
    "\tattributes = record.get('attributes', {})\n",
    "\tproduction_data = attributes.get('productionPerGroupMbaHour', [])\n",
    "\tproduction_data_list.extend(production_data)\n",
    "\n",
    "# Print the first 3 items to check the structure and content\n",
    "# print total number of records\n",
    "# import json\n",
    "# print(json.dumps(production_data_list[:3], indent=4))\n",
    "# print(f\"\\nTotal hourly production records collected: {len(production_data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d04c922-4631-4ce8-a61d-40504a8fe847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully.\n",
      "Keyspace my_keyspace confirmed.\n"
     ]
    }
   ],
   "source": [
    "# Set up Cassandra connection and create keyspace\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.cluster import ConnectionException\n",
    "\n",
    "# IMPORTANT: Ensure your Docker container is running and wait ~90 seconds \n",
    "# before running this cell to allow Cassandra to fully start.\n",
    "try:\n",
    "    # Use 127.0.0.1 to connect to the Docker container from your host machine\n",
    "    cluster = Cluster(['127.0.0.1']) \n",
    "    session = cluster.connect()\n",
    "    print(\"Connection established successfully.\")\n",
    "    \n",
    "    # Create Keyspace\n",
    "    keyspace_query = \"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS CASSANDRA_KEYSPACE \n",
    "        WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\n",
    "    \"\"\"\n",
    "    session.execute(keyspace_query)\n",
    "    print(f\"Keyspace {CASSANDRA_KEYSPACE} confirmed.\")\n",
    "    \n",
    "except ConnectionException as e:\n",
    "    print(f\"ERROR: Failed to connect to Cassandra. Check Docker status. Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0a03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for Spark, Hadoop\n",
    "# Mongo DB setup\n",
    "import streamlit as st\n",
    "try:\n",
    "    MONGO_URI = st.secrets[\"mongo\"][\"uri\"]\n",
    "except KeyError:\n",
    "    st.error(\"MongoDB URI not found in Streamlit secrets. Check your .streamlit/secrets.toml file.\")\n",
    "    st.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697d286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SparkSession for Cassandra and MongoDB\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CassandraToMongo\")\n",
    "    # Cassandra config\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions')\n",
    "    .config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog')\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Spark config\n",
    "spark_master = spark.sparkContext.master\n",
    "spark_conf = spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751d1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark DataFrame from the list\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Define the schema explicitly for robustness\n",
    "schema = StructType([\n",
    "    StructField(\"priceArea\", StringType(), False),\n",
    "    StructField(\"productionGroup\", StringType(), False),\n",
    "    StructField(\"startTime\", StringType(), False),\n",
    "    StructField(\"endTime\", StringType(), False),\n",
    "    StructField(\"lastUpdatedTime\", StringType(), False),\n",
    "    StructField(\"quantityKwh\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Create the Spark DataFrame from the list of dicts\n",
    "df_production_data = spark.createDataFrame(production_data_list, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faad14fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x2812b34df30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new table for production_data (first time only)\n",
    "session.set_keyspace(CASSANDRA_KEYSPACE)\n",
    "session.execute(f\"DROP TABLE IF EXISTS {CASSANDRA_KEYSPACE}.{CASSANDRA_TABLE};\")\n",
    "\n",
    "# Create the table using the structure based on your DataFrame columns\n",
    "session.execute(f\"CREATE TABLE IF NOT EXISTS {CASSANDRA_KEYSPACE}.{CASSANDRA_TABLE}(ind bigint primary key, \\\n",
    "                priceArea text, productiongroup text, startTime text, endTime text, quantityKwh double, lastUpdatedTime text);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ffbbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame Schema:\n",
      "root\n",
      " |-- priceArea: string (nullable = false)\n",
      " |-- productionGroup: string (nullable = false)\n",
      " |-- startTime: string (nullable = false)\n",
      " |-- endTime: string (nullable = false)\n",
      " |-- lastUpdatedTime: string (nullable = false)\n",
      " |-- quantityKwh: double (nullable = false)\n",
      "\n",
      "['priceArea', 'productionGroup', 'startTime', 'endTime', 'lastUpdatedTime', 'quantityKwh']\n",
      "Show the first 5 rows to verify\n",
      "+---------+---------------+--------------------+--------------------+--------------------+-----------+\n",
      "|priceArea|productionGroup|           startTime|             endTime|     lastUpdatedTime|quantityKwh|\n",
      "+---------+---------------+--------------------+--------------------+--------------------+-----------+\n",
      "|      NO1|          hydro|2021-01-01T00:00:...|2021-01-01T01:00:...|2024-12-20T10:35:...|  2507716.8|\n",
      "|      NO1|          hydro|2021-01-01T01:00:...|2021-01-01T02:00:...|2024-12-20T10:35:...|  2494728.0|\n",
      "|      NO1|          hydro|2021-01-01T02:00:...|2021-01-01T03:00:...|2024-12-20T10:35:...|  2486777.5|\n",
      "|      NO1|          hydro|2021-01-01T03:00:...|2021-01-01T04:00:...|2024-12-20T10:35:...|  2461176.0|\n",
      "|      NO1|          hydro|2021-01-01T04:00:...|2021-01-01T05:00:...|2024-12-20T10:35:...|  2466969.2|\n",
      "+---------+---------------+--------------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the DataFrame schema and show a few rows\n",
    "print(\"Spark DataFrame Schema:\")\n",
    "df_production_data.printSchema()\n",
    "print(df_production_data.columns)\n",
    "print(\"Show the first 5 rows to verify\")\n",
    "df_production_data.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15895af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a unique integer ID column named 'ind' to the DataFrame\n",
    "# ind skhould be longType and the primary key in Cassandra \n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "df_final = df_production_data.withColumn(\n",
    "    \"ind\",\n",
    "    monotonically_increasing_id().cast(LongType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e813a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names to lowercase to match Cassandra table schema\n",
    "df_to_write = df_final \\\n",
    "  .withColumnRenamed(\"priceArea\",\"pricearea\") \\\n",
    "  .withColumnRenamed(\"productionGroup\",\"productiongroup\") \\\n",
    "  .withColumnRenamed(\"startTime\",\"starttime\") \\\n",
    "  .withColumnRenamed(\"endTime\",\"endtime\") \\\n",
    "  .withColumnRenamed(\"lastUpdatedTime\",\"lastupdatedtime\") \\\n",
    "  .withColumnRenamed(\"quantityKwh\",\"quantitykwh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bae23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to Cassandra\n",
    "df_to_write.write \\\n",
    "  .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "  .options(keyspace=CASSANDRA_KEYSPACE, table=CASSANDRA_TABLE) \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5a773b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after reading from Cassandra:\n",
      "root\n",
      " |-- ind: long (nullable = false)\n",
      " |-- endtime: string (nullable = true)\n",
      " |-- lastupdatedtime: string (nullable = true)\n",
      " |-- pricearea: string (nullable = true)\n",
      " |-- productiongroup: string (nullable = true)\n",
      " |-- quantitykwh: double (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      "\n",
      "\n",
      "First 5 rows read from Cassandra:\n",
      "+-----------+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "|        ind|             endtime|     lastupdatedtime|pricearea|productiongroup|quantitykwh|           starttime|\n",
      "+-----------+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "|94489294015|2021-11-10T07:00:...|2024-10-27T01:10:...|      NO5|           wind|        0.0|2021-11-10T06:00:...|\n",
      "|       9464|2021-03-07T12:00:...|2024-12-20T10:35:...|      NO1|        thermal|  26976.592|2021-03-07T11:00:...|\n",
      "|60129552114|2021-02-06T11:00:...|2024-12-20T10:35:...|      NO4|          other|      0.053|2021-02-06T10:00:...|\n",
      "|34359751627|2021-12-14T04:00:...|2024-10-27T05:17:...|      NO2|          other|       30.4|2021-12-14T03:00:...|\n",
      "|34359740061|2021-09-18T11:00:...|2024-12-20T10:35:...|      NO2|          hydro|  5791213.0|2021-09-18T10:00:...|\n",
      "+-----------+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total number of rows in the table: 215353\n"
     ]
    }
   ],
   "source": [
    "# Verify data was written to Cassandra by reading it back\n",
    "# Read the data from Cassandra into a DataFrame (df_read)\n",
    "df_read = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=CASSANDRA_TABLE, keyspace=CASSANDRA_KEYSPACE) \\\n",
    "    .load()\n",
    "\n",
    "# Show the schema to confirm types were read correctly\n",
    "print(\"Schema after reading from Cassandra:\")\n",
    "df_read.printSchema()\n",
    "\n",
    "# Show the first 5 rows of data\n",
    "print(\"\\nFirst 5 rows read from Cassandra:\")\n",
    "df_read.show(5)\n",
    "\n",
    "#  Use the .count() action\n",
    "row_count = df_read.count()\n",
    "print(f\"Total number of rows in the table: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efc482-e295-4a18-88e1-7121ed6d14ca",
   "metadata": {},
   "source": [
    "2. Use Spark to extract the columns priceArea, productionGroup, startTime, and quantityKwh from Cassandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3842796c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of Extracted Data:\n",
      "+---------+---------------+--------------------+-----------+\n",
      "|pricearea|productiongroup|           starttime|quantitykwh|\n",
      "+---------+---------------+--------------------+-----------+\n",
      "|      NO1|        thermal|2021-10-17T15:00:...|  29912.145|\n",
      "|      NO3|        thermal|2021-09-21T14:00:...|        0.0|\n",
      "|      NO1|          hydro|2021-05-31T00:00:...|  2736639.2|\n",
      "|      NO3|          solar|2021-08-10T04:00:...|     25.363|\n",
      "|      NO3|        thermal|2021-09-09T12:00:...|        0.0|\n",
      "+---------+---------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copy only columns pricearea, productionsgorup, sarttime and quantitykwh from df_read as extracted dataframe\n",
    "df_extracted = df_read.select(\n",
    "    \"pricearea\",\n",
    "    \"productiongroup\",\n",
    "    \"starttime\",\n",
    "    \"quantitykwh\"\n",
    ")\n",
    "\n",
    "print(\"\\nFirst 5 rows of Extracted Data:\")\n",
    "df_extracted.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da70b9-de08-48f1-9532-d8d25c2ba5eb",
   "metadata": {},
   "source": [
    "3. Create the following plots:  \n",
    "A pie chart for the total production of the year form a choosen price area, where each piece of the pie is one of the production groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6cab64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "from ipywidgets import interact, Dropdown\n",
    "from IPython.display import display\n",
    "\n",
    "# print(\"Starting aggregation...\")\n",
    "\n",
    "# Aggreger alle data ONCE\n",
    "df_agg_all = df_read.groupBy(\"pricearea\", \"productiongroup\") \\\n",
    "                    .sum(\"quantitykwh\") \\\n",
    "                    .withColumnRenamed(\"sum(quantitykwh)\", \"Total_Kwh\")\n",
    "\n",
    "# Convert to Pandas\n",
    "pdf_agg_all = df_agg_all.toPandas()\n",
    "\n",
    "# Get the list of areas\n",
    "price_area_list = pdf_agg_all['pricearea'].unique().tolist()\n",
    "price_area_list.sort()\n",
    "\n",
    "# Add 'Total' as the first choice in the list\n",
    "price_area_list.insert(0, \"Total\") \n",
    "\n",
    "# Get all unique production groups from dataset\n",
    "production_groups = pdf_agg_all['productiongroup'].unique().tolist()\n",
    "production_groups.sort() # Sort for consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f35ae15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting functions\n",
    "# Pre-aggregate the data for all areas to speed up filtering later\n",
    "\n",
    "COLOR_MAP = {\n",
    "    \"hydro\": \"#1f77b4\",   # blue\n",
    "    \"wind\": \"#17becf\",    # cyan/light blue\n",
    "    \"thermal\": \"#7f7f7f\", # gray\n",
    "    \"solar\": \"#e377c2\",   # pink\n",
    "    \"other\": \"#2ca02c\"    # green\n",
    "}\n",
    "\n",
    "def plot_production_pie_fast(selected_area):\n",
    "    \"\"\"\n",
    "    Filters data and draws the pie chart with CONSISTENT colors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data Filtering/Aggregation by pricearea\n",
    "    if selected_area == \"Total\":\n",
    "        pdf = pdf_agg_all.groupby('productiongroup')['Total_Kwh'].sum().reset_index()\n",
    "        title_area = \"All areas in Norway\"\n",
    "    else:\n",
    "        pdf = pdf_agg_all[pdf_agg_all['pricearea'] == selected_area]\n",
    "        title_area = selected_area\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "\n",
    "    if not pdf.empty and pdf['Total_Kwh'].sum() > 0:\n",
    "        # Draw the pie chart\n",
    "        pie_colors = [COLOR_MAP.get(group, \"#AAAAAA\") for group in pdf['productiongroup']]\n",
    "\n",
    "        wedges, texts, autotexts = plt.pie(\n",
    "            pdf['Total_Kwh'], \n",
    "            autopct='%1.1f%%', \n",
    "            startangle=90, \n",
    "            wedgeprops={'edgecolor': 'black'},\n",
    "            textprops={'fontsize' : 18},\n",
    "            pctdistance=0.9,\n",
    "            colors=pie_colors,\n",
    "        )\n",
    "        \n",
    "        # Create labels for the explanation\n",
    "        labels = [f'{l}, {s/pdf[\"Total_Kwh\"].sum():1.1%}' for l, s in zip(pdf['productiongroup'], pdf['Total_Kwh'])]\n",
    "        \n",
    "        # Add the explanation (legend)\n",
    "        plt.legend(\n",
    "            wedges, \n",
    "            labels,\n",
    "            title=\"Produksjonsgruppe, Prosent\",\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1, 0, 0.5, 1)\n",
    "        )\n",
    "\n",
    "        plt.title(f'Total Production per Productiongroup per Pricearea: {title_area}', y=1.05)\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        display(f\"No production data found for pricearea: {title_area}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f5b0afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062a755da48842229f6c14959c3e8442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='selected_area', options=('Total', 'NO1', 'NO2', 'NO3', 'NO4', 'NO5â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "# Get the list of unique price ranges from Pandas DataFrame (pdf_agg_all)\n",
    "price_area_list = pdf_agg_all['pricearea'].unique().tolist()\n",
    "price_area_list.sort()\n",
    "\n",
    "# Add 'Total' as the first choice in the list\n",
    "price_area_list.insert(0, \"Total\") \n",
    "\n",
    "# Create the Dropdown widget\n",
    "area_dropdown = Dropdown(\n",
    "    options=price_area_list,\n",
    "    value=price_area_list[0], # Sets 'Total' as default value\n",
    "    escription='Pricearea:',\n",
    ")\n",
    "\n",
    "# Use 'interact' to connect the dropdown value to the plotting function\n",
    "# The pie chart will automatically update when you select a new range\n",
    "interact(plot_production_pie_fast, selected_area=area_dropdown);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b91980d",
   "metadata": {},
   "source": [
    "A line plot for the first month of the year for a chooen price area. Make separete lines for each production group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c65075c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+-----------+\n",
      "|pricearea|productiongroup|           starttime|quantitykwh|\n",
      "+---------+---------------+--------------------+-----------+\n",
      "|      NO1|           wind|2021-01-30T12:00:...|  78337.055|\n",
      "|      NO2|          solar|2021-01-08T12:00:...|   1022.223|\n",
      "|      NO1|          other|2021-01-26T08:00:...|        0.0|\n",
      "|      NO1|           wind|2021-01-20T08:00:...|  18937.072|\n",
      "|      NO5|          other|2021-01-25T03:00:...|        0.0|\n",
      "+---------+---------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the data for the first month of 2021\n",
    "from pyspark.sql.functions import col, lit, substring\n",
    "\n",
    "df_jan_in_string = df_extracted.filter(\n",
    "    (substring(col(\"starttime\"), 1, 4) == lit(\"2021\")) & \n",
    "    (substring(col(\"starttime\"), 6, 2) == lit(\"01\"))\n",
    ")\n",
    "\n",
    "df_jan_in_string.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de9ffb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "def plot_monthly_production(selected_area):\n",
    "    \"\"\"\n",
    "    Filters the January data by selected range and draws a line graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter/Aggregate based on selected range (\"Total\" or specific pricearea)\n",
    "    if selected_area == \"Total\":\n",
    "        # Aggregate the sum of production for all areas per hour and group\n",
    "        df_plot = df_jan_in_string.groupBy(\"starttime\", \"productiongroup\")\\\n",
    "            .sum(\"quantitykwh\").withColumnRenamed(\"sum(quantitykwh)\", \"quantitykwh\")\n",
    "        title_area = \"All areas in Norway\"\n",
    "    else:\n",
    "        # Filter for the specific area\n",
    "        df_plot = df_jan_in_string.filter(col(\"pricearea\") == selected_area)\n",
    "        title_area = selected_area\n",
    "    \n",
    "    # Sort the data before converting to Pandas\n",
    "    df_plot = df_plot.orderBy(col(\"starttime\"))\n",
    "   \n",
    "    # Convert the plot-ready data to Pandas\n",
    "    pdf = df_plot.toPandas()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "def plot_monthly_production(selected_area):\n",
    "    \"\"\"\n",
    "    Filters the January data by selected range and draws a Dual Y-Axis LINE chart.\n",
    "    Hydro is plotted on the left axis, and all other groups are plotted on the \n",
    "    right axis to resolve the scale imbalance issue.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter/Aggregate based on selected range (\"Total\" or specific pricearea)\n",
    "    if selected_area == \"Total\":\n",
    "        # Aggregate the sum of production for all areas per hour and group\n",
    "        df_plot = df_jan_in_string.groupBy(\"starttime\", \"productiongroup\")\\\n",
    "            .sum(\"quantitykwh\").withColumnRenamed(\"sum(quantitykwh)\", \"quantitykwh\")\n",
    "        title_area = \"All areas in Norway\"\n",
    "    else:\n",
    "        # Filter for the specific area\n",
    "        df_plot = df_jan_in_string.filter(col(\"pricearea\") == selected_area)\n",
    "        title_area = selected_area\n",
    "    \n",
    "    # Sort the data before converting to Pandas\n",
    "    df_plot = df_plot.orderBy(col(\"starttime\"))\n",
    "    \n",
    "    # Convert the plot-ready data to Pandas\n",
    "    pdf = df_plot.toPandas()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Define the dominant group (assuming Hydro is the one crushing the scale)\n",
    "    DOMINANT_GROUP = 'hydro' \n",
    "    \n",
    "    # Lists to store lines and labels for a combined legend\n",
    "    lines = []\n",
    "    labels = []\n",
    "\n",
    "    if not pdf.empty:\n",
    "        \n",
    "        # Ensure 'starttime' is datetime for plotting\n",
    "        if pdf['starttime'].dtype != 'datetime64[ns]':\n",
    "             pdf['starttime'] = pd.to_datetime(pdf['starttime'])\n",
    "\n",
    "        # Create the primary axis (ax1) and secondary axis (ax2)\n",
    "        ax1 = plt.gca() # Primary Y-axis (for Hydro)\n",
    "        ax2 = ax1.twinx() # Secondary Y-axis (for minor groups)\n",
    "\n",
    "        # Iterate over all unique groups\n",
    "        for group in pdf['productiongroup'].unique():\n",
    "            group_data = pdf[pdf['productiongroup'] == group]\n",
    "            current_color = COLOR_MAP.get(group, \"#AAAAAA\")\n",
    "\n",
    "            if group == DOMINANT_GROUP:\n",
    "                # Plot the dominant group on the primary axis (ax1)\n",
    "                line, = ax1.plot(\n",
    "                    group_data['starttime'],\n",
    "                    group_data['quantitykwh'],\n",
    "                    label=group,\n",
    "                    color=current_color,\n",
    "                    linewidth=2.0\n",
    "                )\n",
    "                lines.append(line)\n",
    "                labels.append(group)\n",
    "            else:\n",
    "                # Plot all minor groups on the secondary axis (ax2)\n",
    "                line, = ax2.plot(\n",
    "                    group_data['starttime'],\n",
    "                    group_data['quantitykwh'],\n",
    "                    label=group,\n",
    "                    color=current_color,\n",
    "                    linewidth=1.0,\n",
    "                    linestyle='--' # Use a dashed line for minor groups for visual distinction\n",
    "                )\n",
    "                lines.append(line)\n",
    "                labels.append(group)\n",
    "        \n",
    "        # --- X-AXIS AND TITLE FORMATTING (Fix for clutter) ---\n",
    "        \n",
    "        # Format the X-axis for the primary axis (ax1)\n",
    "        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d'))  # SHOW ONLY DAY NUMBER\n",
    "        ax1.xaxis.set_major_locator(mdates.DayLocator(interval=5)) \n",
    "        ax1.xaxis.set_minor_locator(mdates.DayLocator(interval=1)) \n",
    "        \n",
    "        # Set colors and labels for both Y-axes\n",
    "        ax1.set_ylabel(f'{DOMINANT_GROUP} Production (kWh)', color=COLOR_MAP.get(DOMINANT_GROUP, \"#000000\"))\n",
    "        ax1.tick_params(axis='y', labelcolor=COLOR_MAP.get(DOMINANT_GROUP, \"#000000\"))\n",
    "        \n",
    "        ax2.set_ylabel('Minor Group Production (kWh)', color='gray')\n",
    "        ax2.tick_params(axis='y', labelcolor='gray')\n",
    "        \n",
    "        # Set common axis properties\n",
    "        ax1.set_title(f'Production per Group in January 2021 - Area: {title_area}', fontsize=16)\n",
    "        ax1.set_xlabel(\"Day of Month\") # Reverted label back to just Day\n",
    "        ax1.tick_params(axis='x', rotation=45) # Reduced rotation slightly\n",
    "        \n",
    "        # Add a combined legend using lines and labels collected from both axes\n",
    "        ax1.legend(lines, labels, title=\"Productionsgroup\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        display(f\"No production data found for Area: {title_area} in January 2021.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca6d7fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0328ee8f4fd43549f40471ef541ab22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='PriceArea:', options=('Total', 'NO1', 'NO2', 'NO3', 'NO4', 'NO5'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the Dropdown widget (using the existing list including \"Total\")\n",
    "area_dropdown = Dropdown(\n",
    "    options=price_area_list,\n",
    "    value=price_area_list[0], \n",
    "    description='PriceArea:',\n",
    ")\n",
    "\n",
    "# Connect the dropdown to the plotting function\n",
    "interact(plot_monthly_production, selected_area=area_dropdown);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf5ae1",
   "metadata": {},
   "source": [
    "4. Insert the Spark-extracted data into your MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8af1d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the data into MongoDB\n",
    "df_extracted.write \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"database\", MONGO_DATABASE) \\\n",
    "    .option(\"collection\", MONGO_COLLECTION) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90b34110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- First 5 Rows of Data ---\n",
      "+--------------------+---------+---------------+-----------+--------------------+\n",
      "|                 _id|pricearea|productiongroup|quantitykwh|           starttime|\n",
      "+--------------------+---------+---------------+-----------+--------------------+\n",
      "|68f7e725d80586d43...|      NO5|          hydro|  1387041.0|2021-09-19T08:00:...|\n",
      "|68f7e725d80586d43...|      NO3|          other|      2.187|2021-11-10T16:00:...|\n",
      "|68f7e725d80586d43...|      NO5|           wind|        0.0|2021-10-18T14:00:...|\n",
      "|68f7e725d80586d43...|      NO4|          solar|        0.0|2021-12-29T17:00:...|\n",
      "|68f7e725d80586d43...|      NO4|          solar|      0.493|2021-03-19T15:00:...|\n",
      "+--------------------+---------+---------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify data was written to MongoDB by reading it back\n",
    "try:\n",
    "    df_mongo_data = (\n",
    "        spark.read\n",
    "        .format(\"mongodb\")\n",
    "        # specify the database and collection here, as the connection URI is set globally in the configuration.\n",
    "        .option(\"database\", MONGO_DATABASE)\n",
    "        .option(\"collection\", MONGO_COLLECTION)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    # print(\"\\n Successfully loaded DataFrame from MongoDB Atlas.\")\n",
    "    \n",
    "    #print(\"\\n--- DataFrame Schema (Inferred from MongoDB) ---\")\n",
    "    # df_mongo_data.printSchema()\n",
    "    \n",
    "    print(\"\\n--- First 5 Rows of Data ---\")\n",
    "    df_mongo_data.show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n An error occurred during the read operation.\")\n",
    "    print(f\"Please double-check your database/collection names and network access: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Stop the Spark session when finished\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1321c4-ef54-4cd9-9d20-e8791f966b53",
   "metadata": {},
   "source": [
    "## Tasks, Streamlit app\n",
    "Create a Streamlit app including:\n",
    "- requirements.txt (for package dependencies)  \n",
    "- Four pages (with dummy headers and test contents for now)  \n",
    "- The front/home page should have a sidebar menu with navigation options to the other pages.  \n",
    "-- On the second page:\n",
    "A table showing the imported data (see below). Use the row-wise LineChartColumn() to display the first month of the data series. There should be one row in the table for each column of the imported data.\n",
    "-- On the third page:\n",
    "A plot of the imported data (see below), including header, axis titles and other relevant formatting.  \n",
    "A drop-down menu (st.selectbox) choosing any single column in the CSV or all columns together.\n",
    "A selection slider (st.select_slider) to select a subset of the months. Defaults should be the first month.  \n",
    "\n",
    "Data should be read from a local CSV-file (open-meteo-subset.csv, available in the Files here in Canvas), using caching for app speed.\n",
    "\n",
    "My Streamlit app is \n",
    "https://oriekimura-ind320-projectwork-part.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5a47e",
   "metadata": {},
   "source": [
    "## Log Describing the Compulsory Work  \n",
    "### Reorganizing the Working Folder/File Structure\n",
    "\n",
    "I realized that my folder and file structure for the project was disorganized and inefficient, so I decided to completely reorganize it. The initial chaos resulted from my limited understanding of how to structure a project that integrates multiple tools effectively.\n",
    "\n",
    "I reorganized both the folder structure and the internal code organization within the Jupyter Notebook (.ipynb) and Python (.py) files. Before starting Project 2, I verified that all files functioned correctly across my local PC, GitHub, and Streamlit.io.\n",
    "\n",
    "I did not delete my initial GitHub repository, but from now on, all further project work will be developed within the new structure.\n",
    "\n",
    "Old GitHub link and Streamlit link  \n",
    "https://github.com/oriekimura123/IND320-Projectwork-part1  \n",
    "https://oriekimura-ind320-projectwork-part1.streamlit.app/  \n",
    "\n",
    "\n",
    "New GitHub link and Streamlit link  \n",
    "https://github.com/oriekimura123/IND320-Projectwork  \n",
    "https://oriekimura-ind320-projectwork.streamlit.app/  \n",
    "\n",
    "\n",
    "### Coding in Jupyter Notebook\n",
    "\n",
    "The required tasks were copied and pasted in Markdown format into the notebook. I then wrote and tested the code step by step, using assistance from AI tools.\n",
    "\n",
    "The line chart has two Dual Y-Axis because Gemini recommends me a readable line chart using dual Y-acsis.  \n",
    "\n",
    "\n",
    "I faced significant challenges in several areas:\n",
    "\n",
    "- Finding compatible versions of Cassandra and Spark\n",
    "\n",
    "- Creating a functional Sparkâ€“Cassandraâ€“MongoDB connector\n",
    "\n",
    "- Writing data to Cassandra\n",
    "\n",
    "#### Sparkâ€“Cassandraâ€“MongoDB Connector\n",
    "\n",
    "When I first created the Sparkâ€“Cassandra connector, Spark implicitly used a Docker-related master for Cassandra and did not account for MongoDB also requiring a master connection. This caused several issues that I later resolved to make the combined connector function properly.\n",
    "\n",
    "I placed the following .jar files into C:\\Spark\\spark-3.5.1-bin-hadoop3\\jars:\n",
    "\n",
    "- bson-5.6.1.jar\n",
    "\n",
    "- mongodb-driver-core-5.6.1.jar\n",
    "\n",
    "- mongodb-driver-sync-5.6.1.jar\n",
    "\n",
    "- mongo-spark-connector_2.12-10.5.0.jar\n",
    "\n",
    "I then forced Spark to use local[*] by adding these lines to C:\\Spark\\spark-3.5.1-bin-hadoop3\\conf\\spark-defaults.conf:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9feafb",
   "metadata": {},
   "source": [
    "&nbsp; spark.master local[*]  \n",
    "&nbsp; spark.mongodb.connection.uri mongodb+srv://<user_name>:<password>@<clustername>/?<mongoDBparameters>  \n",
    "&nbsp; spark.mongodb.read.connection.uri mongodb+srv://<user_name>:<password>@<clustername>/?<mongoDBparameters>  \n",
    "&nbsp; spark.mongodb.write.connection.uri mongodb+srv://<user_name>:<password>@<clustername>/?<mongoDBparameters>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291a00d",
   "metadata": {},
   "source": [
    "I also configured the environment variable for the Spark configuration directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10369462",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_CONF_DIR\"] = r\"C:\\Spark\\spark-3.5.1-bin-hadoop3\\conf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f72d2",
   "metadata": {},
   "source": [
    "Then, I set up the environment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a46250fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Setup ---\n",
    "import os\n",
    "\n",
    "SPARK_HOME = \"C:\\\\Spark\\\\spark-3.5.1-bin-hadoop3\"\n",
    "HADOOP_HOME = \"C:\\\\Hadoop\\\\hadoop-3.3.1\"\n",
    "JAVA_HOME = \"C:\\\\Program Files\\\\Microsoft\\\\jdk-21.0.8.9-hotspot\"\n",
    "\n",
    "os.environ.update({\n",
    "    \"SPARK_HOME\": SPARK_HOME,\n",
    "    \"HADOOP_HOME\": HADOOP_HOME,\n",
    "    \"JAVA_HOME\": JAVA_HOME,\n",
    "    \"PATH\": os.environ[\"PATH\"] + os.pathsep + os.path.join(SPARK_HOME, \"bin\"),\n",
    "    \"PYSPARK_PYTHON\": \"python\",\n",
    "    \"PYSPARK_DRIVER_PYTHON\": \"python\",\n",
    "    \"PYSPARK_HADOOP_VERSION\": \"without\",\n",
    "    \"SPARK_CONF_DIR\": os.path.join(SPARK_HOME, \"conf\")\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993a1c6",
   "metadata": {},
   "source": [
    "### Coding the Streamlit App  \n",
    "\n",
    "After completing the core tasks, I ran the application locally. It was challenging to implement filtering logic that accurately reflected user selections across both the pie chart (by pricearea) and the line chart (by productiongroup and month).\n",
    "\n",
    "### Commit and Push to GitHub / Streamlit Cloud Updates  \n",
    "\n",
    "I had to adjust the code to correctly handle both absolute and relative paths to ensure consistent behavior between my local environment and Streamlit Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c1cbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f8c5f7",
   "metadata": {},
   "source": [
    "## Brief Description of AI Usage\n",
    "\n",
    "I used AI tools to generate initial versions of the code, to refine and customize it, and to debug errors.\n",
    "\n",
    "### Coding, Improvement, and Tuning\n",
    "\n",
    "I described my requirements, and AI tools suggested initial drafts. While some of the generated code worked well, others required significant modification.\n",
    "\n",
    "### Debugging\n",
    "I used AI tools frequently for debugging, particularly for issues related to the Spark-Cassandra-MongoDB connector and writing data to Cassandra.  \n",
    "\n",
    "I noticed a key limitation: the error message and the code itself were often insufficient for the AI to solve the problem, leading to failed suggestions.  \n",
    "\n",
    "For example, when I tried to write data to Cassandra, both ChatGPT and Gemini provided many suggestions based on the error code, but none of them fixed the issue. The actual problem was case sensitivity in Cassandra, a detail neither I nor the AI tools initially considered. The AI tools failed to suggest checking case sensitivity and continued offering irrelevant code changes.\n",
    "\n",
    "#### Spark Connection:\n",
    "As mentioned, I had to take numerous steps to get the Spark-Cassandra-MongoDB connector functioning correctly. Both ChatGPT and Gemini provided theoretically correct setup methods for both connectors simultaneously, stating \"it should work.\" However, they failed to account for the specific execution context.  \n",
    "\n",
    "It took several days to begin checking where the .jar files were stored and how the choice of .jar files folder affected execution.   It took another two days to determine the correct location and, finally, one more day to discover how to properly utilize them (by creating a configuration file and referencing it in the Spark session builder). The error codes in VS Code did not provide possible causes for the underlying context issue.  \n",
    "\n",
    "Ultimately, I realized that to use AI tools efficiently, I need to have a deeper, system-level understanding of how the entire code stack operates. It is difficult to know in advance what level of foundational knowledge I must possess and where the limitations of the AI tools lie.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
